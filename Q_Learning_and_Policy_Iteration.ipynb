{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "def train_q_agent(environment, learn_rate, discount_factor, explore_prob, total_episodes):\n",
        "    q_values = np.random.rand(environment.observation_space.n, environment.action_space.n)\n",
        "\n",
        "    for episode in range(total_episodes):\n",
        "        current_state = environment.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if np.random.uniform(0, 1) < explore_prob:\n",
        "                chosen_action = environment.action_space.sample()\n",
        "            else:\n",
        "                chosen_action = np.argmax(q_values[current_state, :])\n",
        "\n",
        "            new_state, reward, done, _ = environment.step(chosen_action)\n",
        "            q_values[current_state, chosen_action] = (1 - learn_rate) * q_values[current_state, chosen_action] + \\\n",
        "                                                     learn_rate * (reward + discount_factor * np.max(q_values[new_state, :]))\n",
        "\n",
        "            current_state = new_state\n",
        "\n",
        "    return q_values\n",
        "\n",
        "def agent_performance(environment, q_values, num_trials=1000):\n",
        "    total_reward = 0\n",
        "    for _ in range(num_trials):\n",
        "        current_state = environment.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = np.argmax(q_values[current_state, :])\n",
        "            new_state, reward, done, _ = environment.step(action)\n",
        "            total_reward += reward\n",
        "            current_state = new_state\n",
        "\n",
        "    avg_reward = total_reward / num_trials\n",
        "    return avg_reward\n",
        "\n",
        "# Create the FrozenLake environment\n",
        "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
        "\n",
        "# Hyperparameters\n",
        "alpha_values = [0.1, 0.5, 0.9]\n",
        "gamma_values = [0.1, 0.5, 0.9]\n",
        "epsilon_values = [0.1, 0.5, 0.9]\n",
        "total_episodes = 10000\n",
        "\n",
        "# Experiment and evaluate different hyperparameter settings\n",
        "for alpha in alpha_values:\n",
        "    for gamma in gamma_values:\n",
        "        for epsilon in epsilon_values:\n",
        "            q_values = train_q_agent(env, alpha, gamma, epsilon, total_episodes)\n",
        "            avg_reward = agent_performance(env, q_values)\n",
        "            print(f\"Alpha: {alpha}, Gamma: {gamma}, Epsilon: {epsilon}, Average Reward: {avg_reward}\")\n",
        "\n",
        "# Close the environment\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdBH1LpKQrw2",
        "outputId": "e1588d58-dcd0-4590-b0df-5392e29d3550"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alpha: 0.1, Gamma: 0.1, Epsilon: 0.1, Average Reward: 0.013\n",
            "Alpha: 0.1, Gamma: 0.1, Epsilon: 0.5, Average Reward: 0.016\n",
            "Alpha: 0.1, Gamma: 0.1, Epsilon: 0.9, Average Reward: 0.019\n",
            "Alpha: 0.1, Gamma: 0.5, Epsilon: 0.1, Average Reward: 0.006\n",
            "Alpha: 0.1, Gamma: 0.5, Epsilon: 0.5, Average Reward: 0.035\n",
            "Alpha: 0.1, Gamma: 0.5, Epsilon: 0.9, Average Reward: 0.016\n",
            "Alpha: 0.1, Gamma: 0.9, Epsilon: 0.1, Average Reward: 0.035\n",
            "Alpha: 0.1, Gamma: 0.9, Epsilon: 0.5, Average Reward: 0.043\n",
            "Alpha: 0.1, Gamma: 0.9, Epsilon: 0.9, Average Reward: 0.061\n",
            "Alpha: 0.5, Gamma: 0.1, Epsilon: 0.1, Average Reward: 0.021\n",
            "Alpha: 0.5, Gamma: 0.1, Epsilon: 0.5, Average Reward: 0.058\n",
            "Alpha: 0.5, Gamma: 0.1, Epsilon: 0.9, Average Reward: 0.0\n",
            "Alpha: 0.5, Gamma: 0.5, Epsilon: 0.1, Average Reward: 0.016\n",
            "Alpha: 0.5, Gamma: 0.5, Epsilon: 0.5, Average Reward: 0.0\n",
            "Alpha: 0.5, Gamma: 0.5, Epsilon: 0.9, Average Reward: 0.0\n",
            "Alpha: 0.5, Gamma: 0.9, Epsilon: 0.1, Average Reward: 0.016\n",
            "Alpha: 0.5, Gamma: 0.9, Epsilon: 0.5, Average Reward: 0.0\n",
            "Alpha: 0.5, Gamma: 0.9, Epsilon: 0.9, Average Reward: 0.133\n",
            "Alpha: 0.9, Gamma: 0.1, Epsilon: 0.1, Average Reward: 0.0\n",
            "Alpha: 0.9, Gamma: 0.1, Epsilon: 0.5, Average Reward: 0.017\n",
            "Alpha: 0.9, Gamma: 0.1, Epsilon: 0.9, Average Reward: 0.017\n",
            "Alpha: 0.9, Gamma: 0.5, Epsilon: 0.1, Average Reward: 0.025\n",
            "Alpha: 0.9, Gamma: 0.5, Epsilon: 0.5, Average Reward: 0.141\n",
            "Alpha: 0.9, Gamma: 0.5, Epsilon: 0.9, Average Reward: 0.042\n",
            "Alpha: 0.9, Gamma: 0.9, Epsilon: 0.1, Average Reward: 0.033\n",
            "Alpha: 0.9, Gamma: 0.9, Epsilon: 0.5, Average Reward: 0.0\n",
            "Alpha: 0.9, Gamma: 0.9, Epsilon: 0.9, Average Reward: 0.019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rmBCZubPQ5Gm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}